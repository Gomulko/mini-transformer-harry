# 🧠 Mini Transformer – My Personal AI Adventure

Hi! 👋 This repo is where I’m building my very own mini language model from scratch — just for fun and learning.  
Think of it as a tiny version of GPT, trained on a small text (like a chapter from Harry Potter), that can guess the next 1–2 words.

## 🚀 Why?

Because I want to *really* understand how large language models (LLMs) like GPT work — not just use them via API.  
This project helps me break things down and build them up again, step by step.

## 🛠️ What I’ll be building

- A simple tokenizer that turns words into numbers
- A small training set based on a short text
- A custom-built Transformer-like model (no fancy frameworks at first)
- A training loop to teach the model to guess what comes next
- Sample predictions to see if it actually works

## 📚 Tools & Stack

- JavaScript or TypeScript (might add Python later)
- No external ML libraries at first — pure logic
- Simple `.txt` files as training data

## 🧪 Learning Goals

- Understand tokenization, embeddings, and attention
- See how a model learns word patterns
- Experiment with context length and architecture
- Build intuition for how LLMs actually "think"

## ✅ Milestones

- [ ] Load text data
- [ ] Build tokenizer
- [ ] Prepare input/output pairs for training
- [ ] Build a tiny neural network (Transformer-ish)
- [ ] Train the model
- [ ] Generate basic predictions

## 📝 Notes

Everything here is experimental and educational.  
I’m documenting the journey as I go, and writing the code in English (as always), but keeping the notes and thoughts in whatever language helps me think better — usually Polish 🇵🇱 😉

---

Let’s see where this goes!
