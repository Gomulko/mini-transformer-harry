# ğŸ§  Mini Transformer â€“ My Personal AI Adventure

Hi! ğŸ‘‹ This repo is where Iâ€™m building my very own mini language model from scratch â€” just for fun and learning.  
Think of it as a tiny version of GPT, trained on a small text (like a chapter from Harry Potter), that can guess the next 1â€“2 words.

## ğŸš€ Why?

Because I want to *really* understand how large language models (LLMs) like GPT work â€” not just use them via API.  
This project helps me break things down and build them up again, step by step.

## ğŸ› ï¸ What Iâ€™ll be building

- A simple tokenizer that turns words into numbers
- A small training set based on a short text
- A custom-built Transformer-like model (no fancy frameworks at first)
- A training loop to teach the model to guess what comes next
- Sample predictions to see if it actually works

## ğŸ“š Tools & Stack

- JavaScript or TypeScript (might add Python later)
- No external ML libraries at first â€” pure logic
- Simple `.txt` files as training data

## ğŸ§ª Learning Goals

- Understand tokenization, embeddings, and attention
- See how a model learns word patterns
- Experiment with context length and architecture
- Build intuition for how LLMs actually "think"

## âœ… Milestones

- [ ] Load text data
- [ ] Build tokenizer
- [ ] Prepare input/output pairs for training
- [ ] Build a tiny neural network (Transformer-ish)
- [ ] Train the model
- [ ] Generate basic predictions

## ğŸ“ Notes

Everything here is experimental and educational.  
Iâ€™m documenting the journey as I go, and writing the code in English (as always), but keeping the notes and thoughts in whatever language helps me think better â€” usually Polish ğŸ‡µğŸ‡± ğŸ˜‰

---

Letâ€™s see where this goes!
